nsu -- тексты с авторами из нгу, 
	level_1 -- level_1 статьи (с абстрактами), всего их 20,933
	all -- все тексты level_1 и level_2 (с абстрактами), всего их 475,221 (на момент 02.04 это все тексты в БД)

sob -- тексты из ИМ СО РАН,
	all -- все тексты level_1 и level_2 (с абстрактами), всего их 50403

в train_data записаны id с абстрактами (очищенными) соответствующих текстов
в doc2vec_models сохранены doc2vec модели по этим текстам


в train_data также добавлены словари с частотами (приставка _freq) слов по всем текстам
для level_1 время составления (и записи): 1.5 сек, для всех (all) -- 32.6 сек
(P.S. метод через составление словаря (list(set(all_words))) оказался нереально долгим: для 1000 текстов он составлялся 50 секунд, когда как основной метод делал это за 0.5 сек, стоит разобраться в регулярных выражениях и попробовать их, для интереса)

